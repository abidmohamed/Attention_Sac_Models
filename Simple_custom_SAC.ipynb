{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87dfd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, distributions as pyd\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import os\n",
    "from collections import deque\n",
    "import random\n",
    "import math\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.spaces import Box, Discrete, Dict\n",
    "from RWGE import WheatGrowthEnv  # Import your custom environment\n",
    "from stable_baselines3 import *\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import gymnasium\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec066ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Dict('PS': Box(0.0, 11.0, (1,), float32), 'accumulated_excess': Box(0.0, 1000.0, (1,), float32), 'accumulated_gdd': Box(0.0, 3000.0, (1,), float32), 'accumulated_scarcity': Box(0.0, 1000.0, (1,), float32), 'current_day': Discrete(365), 'current_month': Discrete(12), 'current_month_day': Discrete(31), 'current_year': Discrete(3000), 'daily_temperature': Box(0.0, 50.0, (1,), float32), 'etc': Box(0.0, 2.0, (1,), float32), 'growth_stage': Box(0.0, 13.0, (1,), float32), 'harvest': Box(0.0, 100.0, (1,), float32), 'humidity': Box(0.0, 100.0, (1,), float32), 'is_cloudy': Discrete(2), 'is_crop_sick': Discrete(2), 'is_raining': Discrete(2), 'qv2m': Box(0.0, 11.0, (1,), float32), 'rainfall': Box(0.0, 11.0, (1,), float32), 'rn_daily': Box(0.0, 1.0, (1,), float32), 'sky_clearness': Box(0.0, 1.0, (1,), float32), 'soil_moisture_content': Box(0.0, 100.0, (1,), float32), 'water_needs': Box(0.0, 11.0, (1,), float32), 'wind_speed': Box(0.0, 11.0, (1,), float32))\n",
      "Action Space: Box(0.0, 11.0, (1,), float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Function to create and return an instance of your custom environment\n",
    "def create_env_fn():\n",
    "    return WheatGrowthEnv(start_day=1, start_month=4, end_day=1, end_month=4, render_mode='human')\n",
    "\n",
    "# Initialize custom environment with make_vec_env\n",
    "env = make_vec_env(create_env_fn, n_envs=1)\n",
    "\n",
    "# Access the observation space and action space\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "# Now you can use your_observation_space and your_action_space as needed\n",
    "print(\"Observation Space:\", observation_space)\n",
    "print(\"Action Space:\", action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "830af2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Discrete: 1\n",
      "Dimension for Discrete: 1\n",
      "Dimension for Discrete: 1\n",
      "Dimension for Discrete: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Discrete: 1\n",
      "Dimension for Discrete: 1\n",
      "Dimension for Discrete: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Dimension for Box: 1\n",
      "Total dimension for Dict: 23\n",
      "Observation Dimension: 23\n",
      "Action Dimension: 1\n",
      "Action Bound: 11.0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 3e-4#1e-4\n",
    "critic_learning_rate = 3e-4#5e-5  # Reduced learning rate for critics\n",
    "tau = 0.005  # for soft update of target parameters\n",
    "gamma = 0.984  # discount factor for future rewards\n",
    "max_grad_norm = 1.0  # Reduced max norm for gradient clipping\n",
    "weight_decay = 1e-5  # L2 regularization strength\n",
    "alpha = 0.01  # Entropy coefficient\n",
    "alpha_lr = 3e-4\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, attention_dim)\n",
    "        self.fc2 = nn.Linear(attention_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = F.relu(self.fc1(x))\n",
    "        scores = self.fc2(scores)\n",
    "        weights = F.softmax(scores, dim=1)\n",
    "        #print(\"Weights\", weights)\n",
    "        output = (x * weights).sum(dim=1)\n",
    "        return output, weights\n",
    "\n",
    "    \n",
    "# Define the Actor Network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, action_bound):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, 64)#(obs_dim, 256)\n",
    "        self.fc2 = nn.Linear(64, 64)#(256, 128)\n",
    "        self.attention = Attention(64, 64)#(128, 128)\n",
    "        self.mean = nn.Linear(64, action_dim)#(128, action_dim)\n",
    "        self.log_std = nn.Linear(64, action_dim)#(128, action_dim)\n",
    "        self.action_bound = action_bound\n",
    "    \n",
    "         # Initialize weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if torch.isnan(x).any():\n",
    "            raise RuntimeError(\"NaN detected after fc1\")\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        if torch.isnan(x).any():\n",
    "            raise RuntimeError(\"NaN detected after fc2\")\n",
    "        \n",
    "        #print(\"X = \", x)\n",
    "        attn_output, _ = self.attention(x.unsqueeze(1))  # Apply attention mechanism\n",
    "        #print(\"Attention = \", attn_output)\n",
    "        mean = self.mean(attn_output)\n",
    "        log_std = self.log_std(attn_output)\n",
    "        #print(\"Mean:\", mean)  # Debugging output\n",
    "        #print(\"Log std before clamp:\", log_std)\n",
    "        log_std = torch.clamp(log_std, min=-4, max=8)  # Clamp for numerical stability\n",
    "        #print(\"Log std after clamp:\", log_std)\n",
    "        std = torch.exp(log_std)\n",
    "        #print(\"Std:\", std)\n",
    "        if torch.isnan(std).any():\n",
    "            raise RuntimeError(\"NaN detected in std calculation, log_std: \" + str(log_std))\n",
    "    \n",
    "        # print(\"Std:\", std)\n",
    "        normal = pyd.Normal(mean, std)\n",
    "        \n",
    "        z = normal.rsample()  # Reparameterization trick\n",
    "        #print(\"Sampled z:\", z)\n",
    "        z_clamped = torch.clamp(z, min=-3, max=3)\n",
    "        \n",
    "        # Scale the output of tanh to the range [0, 11]\n",
    "        action = (torch.tanh(z_clamped) + 1) * 5.5  # Scale and shift tanh output to [0, 11]\n",
    "\n",
    "\n",
    "        # action = torch.tanh(z_clamped) * self.action_bound\n",
    "        #print(\"Action:\", action)\n",
    "        if (action < 0).any() or (action > 11).any():\n",
    "            print(\"Warning: action values out of expected range [0, 11]\")\n",
    "            action = torch.clamp(action, min=0, max=11)\n",
    "\n",
    "        log_prob = normal.log_prob(z) - torch.log(1 - action.pow(2) / 121 + 1e-6)\n",
    "        #print(\"Log Prob:\", log_prob)\n",
    "        \n",
    "        if torch.isnan(log_prob).any():\n",
    "            print(\"NaN in log_prob detected\")\n",
    "            print(\"z:\", z)\n",
    "            print(\"action:\", action)\n",
    "            print(\"1 - action.pow(2) /121:\", 1 - action.pow(2)/121)\n",
    "\n",
    "        if log_prob.dim() > 1:\n",
    "            log_prob = log_prob.sum(-1, keepdim=True)  # Sum across the action dimension if needed\n",
    "        return action, log_prob\n",
    "\n",
    "# Define the Critic Network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim + action_dim, 64)#(obs_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(64, 64)#(256, 128)\n",
    "        self.attention = Attention(64, 64)#(128, 128)\n",
    "        self.fc3 = nn.Linear(64, 1)#(128, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], dim=-1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        attn_output, _ = self.attention(x.unsqueeze(1))  # Apply attention mechanism\n",
    "        value = self.fc3(attn_output)\n",
    "        return value\n",
    "\n",
    "# Function to calculate dimension of observation space\n",
    "def calc_input_shape(space):\n",
    "    if isinstance(space, (gym.spaces.Dict, gymnasium.spaces.Dict)):\n",
    "        total_dim = sum(calc_input_shape(subspace) for _, subspace in space.spaces.items())\n",
    "        print(\"Total dimension for Dict:\", total_dim)  # Diagnostic print\n",
    "        return total_dim\n",
    "    elif isinstance(space, (gym.spaces.Box, gymnasium.spaces.Box)):\n",
    "        box_dim = int(np.prod(space.shape))\n",
    "        print(\"Dimension for Box:\", box_dim)  # Diagnostic print\n",
    "        return box_dim\n",
    "    elif isinstance(space, (gym.spaces.Discrete, gymnasium.spaces.Discrete)):\n",
    "        print(\"Dimension for Discrete: 1\")  # Diagnostic print\n",
    "        return 1\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported space type: {type(space)}\")\n",
    "\n",
    "# Get dimensions\n",
    "# Test and debug the calculation of observation dimensions\n",
    "try:\n",
    "    obs_dim = calc_input_shape(observation_space)\n",
    "except Exception as e:\n",
    "    print(\"Error calculating observation dimensions:\", str(e))\n",
    "\n",
    "action_dim = action_space.shape[0]\n",
    "action_bound = action_space.high[0]\n",
    "\n",
    "print(\"Observation Dimension:\", obs_dim)  # Diagnostic print\n",
    "print(\"Action Dimension:\", action_dim)  # Diagnostic print\n",
    "print(\"Action Bound:\", action_bound)  # Diagnostic print\n",
    "\n",
    "# Define init_weights function outside the class\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "# Initialize networks\n",
    "actor = Actor(obs_dim, action_dim, action_bound).to(device)\n",
    "critic1 = Critic(obs_dim, action_dim).to(device)\n",
    "critic2 = Critic(obs_dim, action_dim).to(device)\n",
    "critic1_target = Critic(obs_dim, action_dim).to(device)\n",
    "critic2_target = Critic(obs_dim, action_dim).to(device)\n",
    "\n",
    "actor.apply(init_weights)\n",
    "critic1.apply(init_weights)\n",
    "critic2.apply(init_weights)\n",
    "critic1_target.apply(init_weights)\n",
    "critic2_target.apply(init_weights)\n",
    "\n",
    "# Copy weights to the target networks\n",
    "critic1_target.load_state_dict(critic1.state_dict())\n",
    "critic2_target.load_state_dict(critic2.state_dict())\n",
    "\n",
    "# Optimizers with L2 regularization\n",
    "actor_optimizer = Adam(actor.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "critic1_optimizer = Adam(critic1.parameters(), lr=critic_learning_rate, weight_decay=weight_decay)\n",
    "critic2_optimizer = Adam(critic2.parameters(), lr=critic_learning_rate, weight_decay=weight_decay)\n",
    "#alpha = torch.tensor(alpha, requires_grad=True)\n",
    "#alpha_optimizer = torch.optim.Adam([alpha], lr=alpha_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6363639d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Episode: 0, Reward: [-19.253574], Mean Reward: -inf\n",
      "-----------------------------------------------\n",
      "Episode: 4, Reward: [-18.41277], Mean Reward: -19.25\n",
      "-----------------------------------------------\n",
      "Episode: 8, Reward: [-18.925385], Mean Reward: -18.83\n",
      "-----------------------------------------------\n",
      "Episode: 12, Reward: [-14.025904], Mean Reward: -18.86\n",
      "-----------------------------------------------\n",
      "Episode: 16, Reward: [-17.208387], Mean Reward: -17.65\n",
      "-----------------------------------------------\n",
      "Episode: 20, Reward: [-15.054116], Mean Reward: -17.57\n",
      "-----------------------------------------------\n",
      "Episode: 24, Reward: [-16.872541], Mean Reward: -17.15\n",
      "-----------------------------------------------\n",
      "Episode: 28, Reward: [-19.267088], Mean Reward: -17.11\n",
      "-----------------------------------------------\n",
      "Episode: 32, Reward: [-20.601084], Mean Reward: -17.38\n",
      "-----------------------------------------------\n",
      "Episode: 36, Reward: [-12.388481], Mean Reward: -17.74\n",
      "-----------------------------------------------\n",
      "Episode: 40, Reward: [-13.615058], Mean Reward: -17.2\n",
      "-----------------------------------------------\n",
      "Episode: 44, Reward: [-19.452509], Mean Reward: -16.87\n",
      "-----------------------------------------------\n",
      "Episode: 48, Reward: [-14.818057], Mean Reward: -17.09\n",
      "-----------------------------------------------\n",
      "Episode: 52, Reward: [-22.182507], Mean Reward: -16.91\n",
      "-----------------------------------------------\n",
      "Episode: 56, Reward: [-13.7541], Mean Reward: -17.29\n",
      "-----------------------------------------------\n",
      "Episode: 60, Reward: [-15.635984], Mean Reward: -17.06\n",
      "-----------------------------------------------\n",
      "Episode: 64, Reward: [-15.815939], Mean Reward: -16.97\n",
      "-----------------------------------------------\n",
      "Episode: 68, Reward: [-13.876185], Mean Reward: -16.9\n",
      "-----------------------------------------------\n",
      "Episode: 72, Reward: [-12.322142], Mean Reward: -16.73\n",
      "-----------------------------------------------\n",
      "Episode: 76, Reward: [-17.081339], Mean Reward: -16.5\n",
      "-----------------------------------------------\n",
      "Episode: 80, Reward: [-17.20887], Mean Reward: -16.53\n",
      "-----------------------------------------------\n",
      "Episode: 84, Reward: [-12.970583], Mean Reward: -16.56\n",
      "-----------------------------------------------\n",
      "Episode: 88, Reward: [-16.989162], Mean Reward: -16.4\n",
      "-----------------------------------------------\n",
      "Episode: 92, Reward: [-15.969342], Mean Reward: -16.42\n",
      "-----------------------------------------------\n",
      "Episode: 96, Reward: [-18.651388], Mean Reward: -16.4\n",
      "-----------------------------------------------\n",
      "Episode: 100, Reward: [-18.154634], Mean Reward: -16.49\n",
      "-----------------------------------------------\n",
      "Episode: 104, Reward: [-16.492554], Mean Reward: -16.56\n",
      "-----------------------------------------------\n",
      "Episode: 108, Reward: [-11.35645], Mean Reward: -16.56\n",
      "-----------------------------------------------\n",
      "Episode: 112, Reward: [-20.872517], Mean Reward: -16.37\n",
      "-----------------------------------------------\n",
      "Episode: 116, Reward: [-15.786033], Mean Reward: -16.53\n",
      "-----------------------------------------------\n",
      "Episode: 120, Reward: [-20.970366], Mean Reward: -16.5\n",
      "-----------------------------------------------\n",
      "Episode: 124, Reward: [-17.901794], Mean Reward: -16.64\n",
      "-----------------------------------------------\n",
      "Episode: 128, Reward: [-12.432672], Mean Reward: -16.68\n",
      "-----------------------------------------------\n",
      "Episode: 132, Reward: [-20.438282], Mean Reward: -16.56\n",
      "-----------------------------------------------\n",
      "Episode: 136, Reward: [-20.655792], Mean Reward: -16.67\n",
      "-----------------------------------------------\n",
      "Episode: 140, Reward: [-11.12344], Mean Reward: -16.78\n",
      "-----------------------------------------------\n",
      "Episode: 144, Reward: [-20.289001], Mean Reward: -16.63\n",
      "-----------------------------------------------\n",
      "Episode: 148, Reward: [-12.186563], Mean Reward: -16.73\n",
      "-----------------------------------------------\n",
      "Episode: 152, Reward: [-12.107582], Mean Reward: -16.61\n",
      "-----------------------------------------------\n",
      "Episode: 156, Reward: [-10.589295], Mean Reward: -16.49\n",
      "-----------------------------------------------\n",
      "Episode: 160, Reward: [-15.199352], Mean Reward: -16.34\n",
      "-----------------------------------------------\n",
      "Episode: 164, Reward: [-13.28385], Mean Reward: -16.31\n",
      "-----------------------------------------------\n",
      "Episode: 168, Reward: [-15.302257], Mean Reward: -16.24\n",
      "-----------------------------------------------\n",
      "Episode: 172, Reward: [-17.04507], Mean Reward: -16.22\n",
      "-----------------------------------------------\n",
      "Episode: 176, Reward: [-14.193956], Mean Reward: -16.24\n",
      "-----------------------------------------------\n",
      "Episode: 180, Reward: [-21.99672], Mean Reward: -16.19\n",
      "-----------------------------------------------\n",
      "Episode: 184, Reward: [-11.9417715], Mean Reward: -16.32\n",
      "-----------------------------------------------\n",
      "Episode: 188, Reward: [-16.055128], Mean Reward: -16.23\n",
      "-----------------------------------------------\n",
      "Episode: 192, Reward: [-13.355814], Mean Reward: -16.22\n",
      "-----------------------------------------------\n",
      "Episode: 196, Reward: [-11.952897], Mean Reward: -16.16\n",
      "-----------------------------------------------\n",
      "Episode: 200, Reward: [-17.484962], Mean Reward: -16.08\n",
      "-----------------------------------------------\n",
      "Episode: 204, Reward: [-11.34627], Mean Reward: -16.11\n",
      "-----------------------------------------------\n",
      "Episode: 208, Reward: [-18.502665], Mean Reward: -16.02\n",
      "-----------------------------------------------\n",
      "Episode: 212, Reward: [-14.091113], Mean Reward: -16.06\n",
      "-----------------------------------------------\n",
      "Episode: 216, Reward: [-14.581738], Mean Reward: -16.03\n",
      "-----------------------------------------------\n",
      "Episode: 220, Reward: [-17.070105], Mean Reward: -16.0\n",
      "-----------------------------------------------\n",
      "Episode: 224, Reward: [-16.212368], Mean Reward: -16.02\n",
      "-----------------------------------------------\n",
      "Episode: 228, Reward: [-11.045583], Mean Reward: -16.02\n",
      "-----------------------------------------------\n",
      "Episode: 232, Reward: [-19.306816], Mean Reward: -15.94\n",
      "-----------------------------------------------\n",
      "Episode: 236, Reward: [-11.612957], Mean Reward: -15.99\n",
      "-----------------------------------------------\n",
      "Episode: 240, Reward: [-12.272161], Mean Reward: -15.92\n",
      "-----------------------------------------------\n",
      "Episode: 244, Reward: [-10.249194], Mean Reward: -15.86\n",
      "-----------------------------------------------\n",
      "Episode: 248, Reward: [-18.553846], Mean Reward: -15.77\n",
      "-----------------------------------------------\n",
      "Episode: 252, Reward: [-10.91795], Mean Reward: -15.82\n",
      "-----------------------------------------------\n",
      "Episode: 256, Reward: [-15.198593], Mean Reward: -15.74\n",
      "-----------------------------------------------\n",
      "Episode: 260, Reward: [-10.708643], Mean Reward: -15.73\n",
      "-----------------------------------------------\n",
      "Episode: 264, Reward: [-10.851009], Mean Reward: -15.65\n",
      "-----------------------------------------------\n",
      "Episode: 268, Reward: [-15.593822], Mean Reward: -15.58\n",
      "-----------------------------------------------\n",
      "Episode: 272, Reward: [-10.599376], Mean Reward: -15.58\n",
      "-----------------------------------------------\n",
      "Episode: 276, Reward: [-17.798306], Mean Reward: -15.51\n",
      "-----------------------------------------------\n",
      "Episode: 280, Reward: [-12.049045], Mean Reward: -15.54\n",
      "-----------------------------------------------\n",
      "Episode: 284, Reward: [-10.46483], Mean Reward: -15.49\n",
      "-----------------------------------------------\n",
      "Episode: 288, Reward: [-9.569108], Mean Reward: -15.42\n",
      "-----------------------------------------------\n",
      "Episode: 292, Reward: [-9.360372], Mean Reward: -15.34\n",
      "-----------------------------------------------\n",
      "Episode: 296, Reward: [-10.082201], Mean Reward: -15.26\n",
      "-----------------------------------------------\n",
      "Episode: 300, Reward: [-12.478027], Mean Reward: -15.19\n",
      "-----------------------------------------------\n",
      "Episode: 304, Reward: [-9.652792], Mean Reward: -15.16\n",
      "-----------------------------------------------\n",
      "Episode: 308, Reward: [-10.724598], Mean Reward: -15.09\n",
      "-----------------------------------------------\n",
      "Episode: 312, Reward: [-14.474277], Mean Reward: -15.03\n",
      "-----------------------------------------------\n",
      "Episode: 316, Reward: [-10.338615], Mean Reward: -15.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Episode: 320, Reward: [-10.106829], Mean Reward: -14.97\n",
      "-----------------------------------------------\n",
      "Episode: 324, Reward: [-10.174215], Mean Reward: -14.91\n",
      "-----------------------------------------------\n",
      "Episode: 328, Reward: [-9.852236], Mean Reward: -14.85\n",
      "-----------------------------------------------\n",
      "Episode: 332, Reward: [-8.468795], Mean Reward: -14.79\n",
      "-----------------------------------------------\n",
      "Episode: 336, Reward: [-11.108016], Mean Reward: -14.71\n",
      "-----------------------------------------------\n",
      "Episode: 340, Reward: [-9.535899], Mean Reward: -14.67\n",
      "-----------------------------------------------\n",
      "Episode: 344, Reward: [-10.17503], Mean Reward: -14.61\n",
      "-----------------------------------------------\n",
      "Episode: 348, Reward: [-10.832778], Mean Reward: -14.56\n",
      "-----------------------------------------------\n",
      "Episode: 352, Reward: [-10.004055], Mean Reward: -14.52\n",
      "-----------------------------------------------\n",
      "Episode: 356, Reward: [-10.854791], Mean Reward: -14.47\n",
      "-----------------------------------------------\n",
      "Episode: 360, Reward: [-9.797179], Mean Reward: -14.43\n",
      "-----------------------------------------------\n",
      "Episode: 364, Reward: [-10.638101], Mean Reward: -14.38\n",
      "-----------------------------------------------\n",
      "Episode: 368, Reward: [-10.721539], Mean Reward: -14.33\n",
      "-----------------------------------------------\n",
      "Episode: 372, Reward: [-10.871829], Mean Reward: -14.3\n",
      "-----------------------------------------------\n",
      "Episode: 376, Reward: [-9.913242], Mean Reward: -14.26\n",
      "-----------------------------------------------\n",
      "Episode: 380, Reward: [-10.208306], Mean Reward: -14.21\n",
      "-----------------------------------------------\n",
      "Episode: 384, Reward: [-7.9709125], Mean Reward: -14.17\n",
      "-----------------------------------------------\n",
      "Episode: 388, Reward: [-8.110969], Mean Reward: -14.11\n",
      "-----------------------------------------------\n",
      "Episode: 392, Reward: [-9.692899], Mean Reward: -14.05\n",
      "-----------------------------------------------\n",
      "Episode: 396, Reward: [-9.433858], Mean Reward: -14.0\n",
      "-----------------------------------------------\n",
      "Episode: 400, Reward: [-9.427969], Mean Reward: -13.96\n",
      "-----------------------------------------------\n",
      "Episode: 404, Reward: [-7.370358], Mean Reward: -13.86\n",
      "-----------------------------------------------\n",
      "Episode: 408, Reward: [-8.90649], Mean Reward: -13.75\n",
      "-----------------------------------------------\n",
      "Episode: 412, Reward: [-8.687169], Mean Reward: -13.65\n",
      "-----------------------------------------------\n",
      "Episode: 416, Reward: [-9.197255], Mean Reward: -13.59\n",
      "-----------------------------------------------\n",
      "Episode: 420, Reward: [-9.31414], Mean Reward: -13.51\n",
      "-----------------------------------------------\n",
      "Episode: 424, Reward: [-9.302166], Mean Reward: -13.46\n",
      "-----------------------------------------------\n",
      "Episode: 428, Reward: [-10.051926], Mean Reward: -13.38\n",
      "-----------------------------------------------\n",
      "Episode: 432, Reward: [-10.869563], Mean Reward: -13.29\n",
      "-----------------------------------------------\n",
      "Episode: 436, Reward: [-8.153265], Mean Reward: -13.19\n",
      "-----------------------------------------------\n",
      "Episode: 440, Reward: [-8.608934], Mean Reward: -13.15\n",
      "-----------------------------------------------\n",
      "Episode: 444, Reward: [-9.73087], Mean Reward: -13.1\n",
      "-----------------------------------------------\n",
      "Episode: 448, Reward: [-8.557399], Mean Reward: -13.0\n",
      "-----------------------------------------------\n",
      "Episode: 452, Reward: [-8.2987585], Mean Reward: -12.94\n",
      "-----------------------------------------------\n",
      "Episode: 456, Reward: [-10.056145], Mean Reward: -12.8\n",
      "-----------------------------------------------\n",
      "Episode: 460, Reward: [-8.832666], Mean Reward: -12.76\n",
      "-----------------------------------------------\n",
      "Episode: 464, Reward: [-9.663145], Mean Reward: -12.7\n",
      "-----------------------------------------------\n",
      "Episode: 468, Reward: [-8.884224], Mean Reward: -12.63\n",
      "-----------------------------------------------\n",
      "Episode: 472, Reward: [-10.384954], Mean Reward: -12.58\n",
      "-----------------------------------------------\n",
      "Episode: 476, Reward: [-8.722282], Mean Reward: -12.57\n",
      "-----------------------------------------------\n",
      "Episode: 480, Reward: [-8.744493], Mean Reward: -12.48\n",
      "-----------------------------------------------\n",
      "Episode: 484, Reward: [-8.543838], Mean Reward: -12.4\n",
      "-----------------------------------------------\n",
      "Episode: 488, Reward: [-9.527165], Mean Reward: -12.35\n",
      "-----------------------------------------------\n",
      "Episode: 492, Reward: [-6.234959], Mean Reward: -12.28\n",
      "-----------------------------------------------\n",
      "Episode: 496, Reward: [-7.038723], Mean Reward: -12.18\n",
      "-----------------------------------------------\n",
      "Episode: 500, Reward: [-8.610565], Mean Reward: -12.06\n",
      "-----------------------------------------------\n",
      "Episode: 504, Reward: [-6.4851794], Mean Reward: -11.97\n",
      "-----------------------------------------------\n",
      "Episode: 508, Reward: [-9.175467], Mean Reward: -11.87\n",
      "-----------------------------------------------\n",
      "Episode: 512, Reward: [-9.353603], Mean Reward: -11.85\n",
      "-----------------------------------------------\n",
      "Episode: 516, Reward: [-5.8394237], Mean Reward: -11.73\n",
      "-----------------------------------------------\n",
      "Episode: 520, Reward: [-7.3557534], Mean Reward: -11.63\n",
      "-----------------------------------------------\n",
      "Episode: 524, Reward: [-5.6791477], Mean Reward: -11.5\n",
      "-----------------------------------------------\n",
      "Episode: 528, Reward: [-12.717044], Mean Reward: -11.37\n",
      "-----------------------------------------------\n",
      "Episode: 532, Reward: [-7.652826], Mean Reward: -11.38\n",
      "-----------------------------------------------\n",
      "Episode: 536, Reward: [-8.650163], Mean Reward: -11.25\n",
      "-----------------------------------------------\n",
      "Episode: 540, Reward: [-8.998129], Mean Reward: -11.13\n",
      "-----------------------------------------------\n",
      "Episode: 544, Reward: [-7.6922035], Mean Reward: -11.11\n",
      "-----------------------------------------------\n",
      "Episode: 548, Reward: [-6.121469], Mean Reward: -10.98\n",
      "-----------------------------------------------\n",
      "Episode: 552, Reward: [-7.8890123], Mean Reward: -10.92\n",
      "-----------------------------------------------\n",
      "Episode: 556, Reward: [-7.601817], Mean Reward: -10.88\n",
      "-----------------------------------------------\n",
      "Episode: 560, Reward: [-7.1457176], Mean Reward: -10.85\n",
      "-----------------------------------------------\n",
      "Episode: 564, Reward: [-5.178237], Mean Reward: -10.77\n",
      "-----------------------------------------------\n",
      "Episode: 568, Reward: [-6.484466], Mean Reward: -10.69\n",
      "-----------------------------------------------\n",
      "Episode: 572, Reward: [-8.517601], Mean Reward: -10.6\n",
      "-----------------------------------------------\n",
      "Episode: 576, Reward: [-7.6996765], Mean Reward: -10.51\n",
      "-----------------------------------------------\n",
      "Episode: 580, Reward: [-5.0611997], Mean Reward: -10.45\n",
      "-----------------------------------------------\n",
      "Episode: 584, Reward: [-5.8253174], Mean Reward: -10.28\n",
      "-----------------------------------------------\n",
      "Episode: 588, Reward: [-8.089511], Mean Reward: -10.22\n",
      "-----------------------------------------------\n",
      "Episode: 592, Reward: [-4.108407], Mean Reward: -10.14\n",
      "-----------------------------------------------\n",
      "Episode: 596, Reward: [-6.7762628], Mean Reward: -10.05\n",
      "-----------------------------------------------\n",
      "Episode: 600, Reward: [-6.3002596], Mean Reward: -9.99\n",
      "-----------------------------------------------\n",
      "Episode: 604, Reward: [-7.1718636], Mean Reward: -9.88\n",
      "-----------------------------------------------\n",
      "Episode: 608, Reward: [-5.226332], Mean Reward: -9.84\n",
      "-----------------------------------------------\n",
      "Episode: 612, Reward: [-6.120128], Mean Reward: -9.71\n",
      "-----------------------------------------------\n",
      "Episode: 616, Reward: [-5.8348517], Mean Reward: -9.63\n",
      "-----------------------------------------------\n",
      "Episode: 620, Reward: [-6.4088345], Mean Reward: -9.54\n",
      "-----------------------------------------------\n",
      "Episode: 624, Reward: [-5.848178], Mean Reward: -9.43\n",
      "-----------------------------------------------\n",
      "Episode: 628, Reward: [-6.560625], Mean Reward: -9.33\n",
      "-----------------------------------------------\n",
      "Episode: 632, Reward: [-10.645556], Mean Reward: -9.29\n",
      "-----------------------------------------------\n",
      "Episode: 636, Reward: [-5.5927033], Mean Reward: -9.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Episode: 640, Reward: [-6.3316693], Mean Reward: -9.14\n",
      "-----------------------------------------------\n",
      "Episode: 644, Reward: [-7.8338957], Mean Reward: -9.08\n",
      "-----------------------------------------------\n",
      "Episode: 648, Reward: [-8.4520645], Mean Reward: -9.06\n",
      "-----------------------------------------------\n",
      "Episode: 652, Reward: [-5.741521], Mean Reward: -8.95\n",
      "-----------------------------------------------\n",
      "Episode: 656, Reward: [-8.214854], Mean Reward: -8.9\n",
      "-----------------------------------------------\n",
      "Episode: 660, Reward: [-8.5840845], Mean Reward: -8.83\n",
      "-----------------------------------------------\n",
      "Episode: 664, Reward: [-5.646374], Mean Reward: -8.81\n",
      "-----------------------------------------------\n",
      "Episode: 668, Reward: [-7.9257245], Mean Reward: -8.76\n",
      "-----------------------------------------------\n",
      "Episode: 672, Reward: [-6.2174206], Mean Reward: -8.68\n",
      "-----------------------------------------------\n",
      "Episode: 676, Reward: [-7.3620367], Mean Reward: -8.64\n",
      "-----------------------------------------------\n",
      "Episode: 680, Reward: [-3.3889856], Mean Reward: -8.54\n",
      "-----------------------------------------------\n",
      "Episode: 684, Reward: [-8.226927], Mean Reward: -8.45\n",
      "-----------------------------------------------\n",
      "Episode: 688, Reward: [-7.079448], Mean Reward: -8.43\n",
      "-----------------------------------------------\n",
      "Episode: 692, Reward: [-5.479878], Mean Reward: -8.4\n",
      "-----------------------------------------------\n",
      "Episode: 696, Reward: [-8.904416], Mean Reward: -8.36\n",
      "-----------------------------------------------\n",
      "Episode: 700, Reward: [-4.8896546], Mean Reward: -8.35\n",
      "-----------------------------------------------\n",
      "Episode: 704, Reward: [-3.2371626], Mean Reward: -8.27\n",
      "-----------------------------------------------\n",
      "Episode: 708, Reward: [-4.922919], Mean Reward: -8.21\n",
      "-----------------------------------------------\n",
      "Episode: 712, Reward: [-3.6904504], Mean Reward: -8.15\n",
      "-----------------------------------------------\n",
      "Episode: 716, Reward: [-7.7247777], Mean Reward: -8.04\n",
      "-----------------------------------------------\n",
      "Episode: 720, Reward: [-7.153403], Mean Reward: -8.02\n",
      "-----------------------------------------------\n",
      "Episode: 724, Reward: [-7.9065166], Mean Reward: -7.99\n",
      "-----------------------------------------------\n",
      "Episode: 728, Reward: [-3.9387765], Mean Reward: -7.97\n",
      "-----------------------------------------------\n",
      "Episode: 732, Reward: [-2.877942], Mean Reward: -7.91\n",
      "-----------------------------------------------\n",
      "Episode: 736, Reward: [-2.3016517], Mean Reward: -7.85\n",
      "-----------------------------------------------\n",
      "Episode: 740, Reward: [-2.2919657], Mean Reward: -7.76\n",
      "-----------------------------------------------\n",
      "Episode: 744, Reward: [-5.587029], Mean Reward: -7.69\n",
      "-----------------------------------------------\n",
      "Episode: 748, Reward: [-2.0922782], Mean Reward: -7.64\n",
      "-----------------------------------------------\n",
      "Episode: 752, Reward: [-2.9020743], Mean Reward: -7.56\n",
      "-----------------------------------------------\n",
      "Episode: 756, Reward: [-7.264674], Mean Reward: -7.49\n",
      "-----------------------------------------------\n",
      "Episode: 760, Reward: [-5.360008], Mean Reward: -7.45\n",
      "-----------------------------------------------\n",
      "Episode: 764, Reward: [-2.8451014], Mean Reward: -7.41\n",
      "-----------------------------------------------\n",
      "Episode: 768, Reward: [-6.214678], Mean Reward: -7.33\n",
      "-----------------------------------------------\n",
      "Episode: 772, Reward: [-7.8143263], Mean Reward: -7.28\n",
      "-----------------------------------------------\n",
      "Episode: 776, Reward: [-5.2284055], Mean Reward: -7.25\n",
      "-----------------------------------------------\n",
      "Episode: 780, Reward: [-9.99321], Mean Reward: -7.21\n",
      "-----------------------------------------------\n",
      "Episode: 784, Reward: [-4.4592943], Mean Reward: -7.2\n",
      "-----------------------------------------------\n",
      "Episode: 788, Reward: [-7.804491], Mean Reward: -7.17\n",
      "-----------------------------------------------\n",
      "Episode: 792, Reward: [-6.189722], Mean Reward: -7.17\n",
      "-----------------------------------------------\n",
      "Episode: 796, Reward: [0.42012882], Mean Reward: -7.13\n",
      "-----------------------------------------------\n",
      "Episode: 800, Reward: [-1.8421311], Mean Reward: -7.03\n",
      "-----------------------------------------------\n",
      "Episode: 804, Reward: [-4.726249], Mean Reward: -6.96\n",
      "-----------------------------------------------\n",
      "Episode: 808, Reward: [-3.2196412], Mean Reward: -6.93\n",
      "-----------------------------------------------\n",
      "Episode: 812, Reward: [-3.0884612], Mean Reward: -6.87\n",
      "-----------------------------------------------\n",
      "Episode: 816, Reward: [-6.1104336], Mean Reward: -6.82\n",
      "-----------------------------------------------\n",
      "Episode: 820, Reward: [-6.372877], Mean Reward: -6.79\n",
      "-----------------------------------------------\n",
      "Episode: 824, Reward: [-4.5824075], Mean Reward: -6.76\n",
      "-----------------------------------------------\n",
      "Episode: 828, Reward: [-4.6844163], Mean Reward: -6.71\n",
      "-----------------------------------------------\n",
      "Episode: 832, Reward: [-3.9567409], Mean Reward: -6.66\n",
      "-----------------------------------------------\n",
      "Episode: 836, Reward: [-0.44799507], Mean Reward: -6.59\n",
      "-----------------------------------------------\n",
      "Episode: 840, Reward: [-0.37016696], Mean Reward: -6.51\n",
      "-----------------------------------------------\n",
      "Episode: 844, Reward: [-5.484094], Mean Reward: -6.43\n",
      "-----------------------------------------------\n",
      "Episode: 848, Reward: [-6.8859453], Mean Reward: -6.38\n",
      "-----------------------------------------------\n",
      "Episode: 852, Reward: [-6.609648], Mean Reward: -6.37\n",
      "-----------------------------------------------\n",
      "Episode: 856, Reward: [-9.902987], Mean Reward: -6.35\n",
      "-----------------------------------------------\n",
      "Episode: 860, Reward: [-6.828266], Mean Reward: -6.35\n",
      "-----------------------------------------------\n",
      "Episode: 864, Reward: [-8.69095], Mean Reward: -6.33\n",
      "-----------------------------------------------\n",
      "Episode: 868, Reward: [-3.1993906], Mean Reward: -6.32\n",
      "-----------------------------------------------\n",
      "Episode: 872, Reward: [-2.9031358], Mean Reward: -6.26\n",
      "-----------------------------------------------\n",
      "Episode: 876, Reward: [-3.008421], Mean Reward: -6.19\n",
      "-----------------------------------------------\n",
      "Episode: 880, Reward: [-6.1010985], Mean Reward: -6.13\n",
      "-----------------------------------------------\n",
      "Episode: 884, Reward: [-3.6587992], Mean Reward: -6.1\n",
      "-----------------------------------------------\n",
      "Episode: 888, Reward: [-3.5712037], Mean Reward: -6.06\n",
      "-----------------------------------------------\n",
      "Episode: 892, Reward: [-4.8005266], Mean Reward: -6.0\n",
      "-----------------------------------------------\n",
      "Episode: 896, Reward: [-5.6755276], Mean Reward: -5.98\n",
      "-----------------------------------------------\n",
      "Episode: 900, Reward: [-4.528125], Mean Reward: -5.97\n",
      "-----------------------------------------------\n",
      "Episode: 904, Reward: [-6.330385], Mean Reward: -5.93\n",
      "-----------------------------------------------\n",
      "Episode: 908, Reward: [-2.2396684], Mean Reward: -5.93\n",
      "-----------------------------------------------\n",
      "Episode: 912, Reward: [-2.1549988], Mean Reward: -5.86\n",
      "-----------------------------------------------\n",
      "Episode: 916, Reward: [-2.9024558], Mean Reward: -5.78\n",
      "-----------------------------------------------\n",
      "Episode: 920, Reward: [-7.176182], Mean Reward: -5.75\n",
      "-----------------------------------------------\n",
      "Episode: 924, Reward: [-2.186811], Mean Reward: -5.75\n",
      "-----------------------------------------------\n",
      "Episode: 928, Reward: [-4.6172514], Mean Reward: -5.72\n",
      "-----------------------------------------------\n",
      "Episode: 932, Reward: [-6.8945603], Mean Reward: -5.64\n",
      "-----------------------------------------------\n",
      "Episode: 936, Reward: [-4.593479], Mean Reward: -5.63\n",
      "-----------------------------------------------\n",
      "Episode: 940, Reward: [-5.366762], Mean Reward: -5.59\n",
      "-----------------------------------------------\n",
      "Episode: 944, Reward: [-4.9043064], Mean Reward: -5.55\n",
      "-----------------------------------------------\n",
      "Episode: 948, Reward: [0.8873103], Mean Reward: -5.52\n",
      "-----------------------------------------------\n",
      "Episode: 952, Reward: [-7.1391587], Mean Reward: -5.45\n",
      "-----------------------------------------------\n",
      "Episode: 956, Reward: [-7.0665827], Mean Reward: -5.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Episode: 960, Reward: [-3.0615635], Mean Reward: -5.44\n",
      "-----------------------------------------------\n",
      "Episode: 964, Reward: [-7.44373], Mean Reward: -5.4\n",
      "-----------------------------------------------\n",
      "Episode: 968, Reward: [-2.8386636], Mean Reward: -5.42\n",
      "-----------------------------------------------\n",
      "Episode: 972, Reward: [-5.9520936], Mean Reward: -5.39\n",
      "-----------------------------------------------\n",
      "Episode: 976, Reward: [-1.277725], Mean Reward: -5.36\n",
      "-----------------------------------------------\n",
      "Episode: 980, Reward: [-3.5670488], Mean Reward: -5.3\n",
      "-----------------------------------------------\n",
      "Episode: 984, Reward: [2.05996], Mean Reward: -5.28\n",
      "-----------------------------------------------\n",
      "Episode: 988, Reward: [-5.3991175], Mean Reward: -5.2\n",
      "-----------------------------------------------\n",
      "Episode: 992, Reward: [-2.8109436], Mean Reward: -5.18\n",
      "-----------------------------------------------\n",
      "Episode: 996, Reward: [-1.9701698], Mean Reward: -5.16\n",
      "-----------------------------------------------\n",
      "Episode: 1000, Reward: [0.48732752], Mean Reward: -5.12\n",
      "-----------------------------------------------\n",
      "Episode: 1004, Reward: [-2.3465905], Mean Reward: -5.05\n",
      "-----------------------------------------------\n",
      "Episode: 1008, Reward: [-4.6007133], Mean Reward: -5.0\n",
      "-----------------------------------------------\n",
      "Episode: 1012, Reward: [-6.0951962], Mean Reward: -4.99\n",
      "-----------------------------------------------\n",
      "Episode: 1016, Reward: [-2.910473], Mean Reward: -4.99\n",
      "-----------------------------------------------\n",
      "Episode: 1020, Reward: [-5.3936634], Mean Reward: -4.96\n",
      "-----------------------------------------------\n",
      "Episode: 1024, Reward: [0.25858968], Mean Reward: -4.95\n",
      "-----------------------------------------------\n",
      "Episode: 1028, Reward: [-3.155387], Mean Reward: -4.89\n",
      "-----------------------------------------------\n",
      "Episode: 1032, Reward: [-3.708965], Mean Reward: -4.86\n",
      "-----------------------------------------------\n",
      "Episode: 1036, Reward: [-6.3213263], Mean Reward: -4.79\n",
      "-----------------------------------------------\n",
      "Episode: 1040, Reward: [-1.5870678], Mean Reward: -4.8\n",
      "-----------------------------------------------\n",
      "Episode: 1044, Reward: [-3.930296], Mean Reward: -4.75\n",
      "-----------------------------------------------\n",
      "Episode: 1048, Reward: [-1.2988575], Mean Reward: -4.71\n",
      "-----------------------------------------------\n",
      "Episode: 1052, Reward: [-3.8385043], Mean Reward: -4.64\n",
      "-----------------------------------------------\n",
      "Episode: 1056, Reward: [-0.7006616], Mean Reward: -4.62\n",
      "-----------------------------------------------\n",
      "Episode: 1060, Reward: [-1.0427101], Mean Reward: -4.54\n",
      "-----------------------------------------------\n",
      "Episode: 1064, Reward: [-2.1132646], Mean Reward: -4.47\n",
      "-----------------------------------------------\n",
      "Episode: 1068, Reward: [-3.4376605], Mean Reward: -4.43\n",
      "-----------------------------------------------\n",
      "Episode: 1072, Reward: [-4.4131274], Mean Reward: -4.39\n",
      "-----------------------------------------------\n",
      "Episode: 1076, Reward: [-2.130347], Mean Reward: -4.37\n",
      "-----------------------------------------------\n",
      "Episode: 1080, Reward: [1.1422935], Mean Reward: -4.32\n",
      "-----------------------------------------------\n",
      "Episode: 1084, Reward: [1.1306946], Mean Reward: -4.27\n",
      "-----------------------------------------------\n",
      "Episode: 1088, Reward: [0.48005378], Mean Reward: -4.18\n",
      "-----------------------------------------------\n",
      "Episode: 1092, Reward: [2.717686], Mean Reward: -4.1\n",
      "-----------------------------------------------\n",
      "Episode: 1096, Reward: [-1.5843403], Mean Reward: -4.02\n",
      "-----------------------------------------------\n",
      "Episode: 1100, Reward: [-1.1941565], Mean Reward: -3.95\n",
      "-----------------------------------------------\n",
      "Episode: 1104, Reward: [-2.1797833], Mean Reward: -3.91\n",
      "-----------------------------------------------\n",
      "Episode: 1108, Reward: [-4.496334], Mean Reward: -3.9\n",
      "-----------------------------------------------\n",
      "Episode: 1112, Reward: [-1.034172], Mean Reward: -3.9\n",
      "-----------------------------------------------\n",
      "Episode: 1116, Reward: [1.1969979], Mean Reward: -3.87\n",
      "-----------------------------------------------\n",
      "Episode: 1120, Reward: [-7.8964577], Mean Reward: -3.78\n",
      "-----------------------------------------------\n",
      "Episode: 1124, Reward: [-5.0970807], Mean Reward: -3.79\n",
      "-----------------------------------------------\n",
      "Episode: 1128, Reward: [0.22860348], Mean Reward: -3.76\n",
      "-----------------------------------------------\n",
      "Episode: 1132, Reward: [-4.0036707], Mean Reward: -3.72\n",
      "-----------------------------------------------\n",
      "Episode: 1136, Reward: [-6.523656], Mean Reward: -3.73\n",
      "-----------------------------------------------\n",
      "Episode: 1140, Reward: [-2.350863], Mean Reward: -3.77\n",
      "-----------------------------------------------\n",
      "Episode: 1144, Reward: [-4.389941], Mean Reward: -3.77\n",
      "-----------------------------------------------\n",
      "Episode: 1148, Reward: [-2.1776965], Mean Reward: -3.76\n",
      "-----------------------------------------------\n",
      "Episode: 1152, Reward: [-4.6079025], Mean Reward: -3.76\n",
      "-----------------------------------------------\n",
      "Episode: 1156, Reward: [-0.41395772], Mean Reward: -3.78\n",
      "-----------------------------------------------\n",
      "Episode: 1160, Reward: [-7.6103015], Mean Reward: -3.71\n",
      "-----------------------------------------------\n",
      "Episode: 1164, Reward: [-2.860303], Mean Reward: -3.73\n",
      "-----------------------------------------------\n",
      "Episode: 1168, Reward: [-1.4000684], Mean Reward: -3.73\n",
      "-----------------------------------------------\n",
      "Episode: 1172, Reward: [-2.7149832], Mean Reward: -3.68\n",
      "-----------------------------------------------\n",
      "Episode: 1176, Reward: [0.9203259], Mean Reward: -3.63\n",
      "-----------------------------------------------\n",
      "Episode: 1180, Reward: [-0.5662708], Mean Reward: -3.57\n",
      "-----------------------------------------------\n",
      "Episode: 1184, Reward: [-2.8141062], Mean Reward: -3.48\n",
      "-----------------------------------------------\n",
      "Episode: 1188, Reward: [-1.132266], Mean Reward: -3.46\n",
      "-----------------------------------------------\n",
      "Episode: 1192, Reward: [-3.1310394], Mean Reward: -3.39\n",
      "-----------------------------------------------\n",
      "Episode: 1196, Reward: [-1.9649016], Mean Reward: -3.36\n",
      "-----------------------------------------------\n",
      "Episode: 1200, Reward: [-4.5297346], Mean Reward: -3.39\n",
      "-----------------------------------------------\n",
      "Episode: 1204, Reward: [-3.5639238], Mean Reward: -3.41\n",
      "-----------------------------------------------\n",
      "Episode: 1208, Reward: [0.51510364], Mean Reward: -3.4\n",
      "-----------------------------------------------\n",
      "Episode: 1212, Reward: [-1.3033581], Mean Reward: -3.37\n",
      "-----------------------------------------------\n",
      "Episode: 1216, Reward: [-3.6377912], Mean Reward: -3.35\n",
      "-----------------------------------------------\n",
      "Episode: 1220, Reward: [-5.2711406], Mean Reward: -3.32\n",
      "-----------------------------------------------\n",
      "Episode: 1224, Reward: [-3.7894576], Mean Reward: -3.31\n",
      "-----------------------------------------------\n",
      "Episode: 1228, Reward: [-2.9222372], Mean Reward: -3.3\n",
      "-----------------------------------------------\n",
      "Episode: 1232, Reward: [-4.0276875], Mean Reward: -3.29\n",
      "-----------------------------------------------\n",
      "Episode: 1236, Reward: [-2.0939045], Mean Reward: -3.29\n",
      "-----------------------------------------------\n",
      "Episode: 1240, Reward: [-4.5619187], Mean Reward: -3.3\n",
      "-----------------------------------------------\n",
      "Episode: 1244, Reward: [-3.0702293], Mean Reward: -3.35\n",
      "-----------------------------------------------\n",
      "Episode: 1248, Reward: [-5.3519664], Mean Reward: -3.32\n",
      "-----------------------------------------------\n",
      "Episode: 1252, Reward: [1.3754008], Mean Reward: -3.31\n",
      "-----------------------------------------------\n",
      "Episode: 1256, Reward: [-3.8656018], Mean Reward: -3.23\n",
      "-----------------------------------------------\n",
      "Episode: 1260, Reward: [1.1171536], Mean Reward: -3.17\n",
      "-----------------------------------------------\n",
      "Episode: 1264, Reward: [-1.8908467], Mean Reward: -3.09\n",
      "-----------------------------------------------\n",
      "Episode: 1268, Reward: [-5.194007], Mean Reward: -3.02\n",
      "-----------------------------------------------\n",
      "Episode: 1272, Reward: [-3.8476439], Mean Reward: -3.04\n",
      "-----------------------------------------------\n",
      "Episode: 1276, Reward: [-4.329772], Mean Reward: -3.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Episode: 1280, Reward: [-2.0018826], Mean Reward: -3.06\n",
      "-----------------------------------------------\n",
      "Episode: 1284, Reward: [-3.7838535], Mean Reward: -3.02\n",
      "-----------------------------------------------\n",
      "Episode: 1288, Reward: [-5.285997], Mean Reward: -3.02\n",
      "-----------------------------------------------\n",
      "Episode: 1292, Reward: [-4.7672596], Mean Reward: -3.04\n",
      "-----------------------------------------------\n",
      "Episode: 1296, Reward: [-4.335012], Mean Reward: -3.04\n",
      "-----------------------------------------------\n",
      "Episode: 1300, Reward: [0.02135301], Mean Reward: -3.02\n",
      "-----------------------------------------------\n",
      "Episode: 1304, Reward: [-5.252964], Mean Reward: -2.98\n",
      "-----------------------------------------------\n",
      "Episode: 1308, Reward: [-2.8638656], Mean Reward: -2.97\n",
      "-----------------------------------------------\n",
      "Episode: 1312, Reward: [1.1035016], Mean Reward: -2.97\n",
      "-----------------------------------------------\n",
      "Episode: 1316, Reward: [-2.1010308], Mean Reward: -2.94\n",
      "-----------------------------------------------\n",
      "Episode: 1320, Reward: [-4.448448], Mean Reward: -2.93\n",
      "-----------------------------------------------\n",
      "Episode: 1324, Reward: [-0.29660875], Mean Reward: -2.91\n",
      "-----------------------------------------------\n",
      "Episode: 1328, Reward: [-1.0411477], Mean Reward: -2.89\n",
      "-----------------------------------------------\n",
      "Episode: 1332, Reward: [-2.0644908], Mean Reward: -2.85\n",
      "-----------------------------------------------\n",
      "Episode: 1336, Reward: [-1.1543872], Mean Reward: -2.8\n",
      "-----------------------------------------------\n",
      "Episode: 1340, Reward: [2.7903295], Mean Reward: -2.77\n",
      "-----------------------------------------------\n",
      "Episode: 1344, Reward: [-5.55042], Mean Reward: -2.69\n",
      "-----------------------------------------------\n",
      "Episode: 1348, Reward: [-4.58968], Mean Reward: -2.69\n",
      "-----------------------------------------------\n",
      "Episode: 1352, Reward: [-6.156784], Mean Reward: -2.75\n",
      "-----------------------------------------------\n",
      "Episode: 1356, Reward: [-2.4102888], Mean Reward: -2.74\n",
      "-----------------------------------------------\n",
      "Episode: 1360, Reward: [-3.7812383], Mean Reward: -2.69\n",
      "-----------------------------------------------\n",
      "Episode: 1364, Reward: [3.1275992], Mean Reward: -2.7\n",
      "-----------------------------------------------\n",
      "Episode: 1368, Reward: [-1.2521417], Mean Reward: -2.59\n",
      "-----------------------------------------------\n",
      "Episode: 1372, Reward: [-4.5749125], Mean Reward: -2.58\n",
      "-----------------------------------------------\n",
      "Episode: 1376, Reward: [-3.2814252], Mean Reward: -2.56\n",
      "-----------------------------------------------\n",
      "Episode: 1380, Reward: [-3.5094538], Mean Reward: -2.58\n",
      "-----------------------------------------------\n",
      "Episode: 1384, Reward: [-1.2245295], Mean Reward: -2.58\n",
      "-----------------------------------------------\n",
      "Episode: 1388, Reward: [-0.33859408], Mean Reward: -2.62\n",
      "-----------------------------------------------\n",
      "Episode: 1392, Reward: [-4.4935603], Mean Reward: -2.57\n",
      "-----------------------------------------------\n",
      "Episode: 1396, Reward: [1.0984135], Mean Reward: -2.58\n",
      "-----------------------------------------------\n",
      "Episode: 1400, Reward: [-7.509014], Mean Reward: -2.55\n",
      "-----------------------------------------------\n",
      "Episode: 1404, Reward: [-4.733702], Mean Reward: -2.63\n",
      "-----------------------------------------------\n",
      "Episode: 1408, Reward: [-5.4780884], Mean Reward: -2.66\n",
      "-----------------------------------------------\n",
      "Episode: 1412, Reward: [-1.3299541], Mean Reward: -2.67\n",
      "-----------------------------------------------\n",
      "Episode: 1416, Reward: [-2.2892437], Mean Reward: -2.62\n",
      "-----------------------------------------------\n",
      "Episode: 1420, Reward: [-3.6720815], Mean Reward: -2.61\n",
      "-----------------------------------------------\n",
      "Episode: 1424, Reward: [3.132233], Mean Reward: -2.59\n",
      "-----------------------------------------------\n",
      "Episode: 1428, Reward: [-5.2753887], Mean Reward: -2.57\n",
      "-----------------------------------------------\n",
      "Episode: 1432, Reward: [2.930797], Mean Reward: -2.59\n",
      "-----------------------------------------------\n",
      "Episode: 1436, Reward: [-1.0038648], Mean Reward: -2.52\n",
      "-----------------------------------------------\n",
      "Episode: 1440, Reward: [-5.698076], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1444, Reward: [-2.971673], Mean Reward: -2.51\n",
      "-----------------------------------------------\n",
      "Episode: 1448, Reward: [1.3145578], Mean Reward: -2.5\n",
      "-----------------------------------------------\n",
      "Episode: 1452, Reward: [-2.9132214], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1456, Reward: [-3.9347632], Mean Reward: -2.46\n",
      "-----------------------------------------------\n",
      "Episode: 1460, Reward: [-1.0285999], Mean Reward: -2.5\n",
      "-----------------------------------------------\n",
      "Episode: 1464, Reward: [-0.3273964], Mean Reward: -2.5\n",
      "-----------------------------------------------\n",
      "Episode: 1468, Reward: [-2.2131884], Mean Reward: -2.48\n",
      "-----------------------------------------------\n",
      "Episode: 1472, Reward: [-3.9899054], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1476, Reward: [-1.2414958], Mean Reward: -2.46\n",
      "-----------------------------------------------\n",
      "Episode: 1480, Reward: [-2.3223565], Mean Reward: -2.45\n",
      "-----------------------------------------------\n",
      "Episode: 1484, Reward: [-2.9513965], Mean Reward: -2.49\n",
      "-----------------------------------------------\n",
      "Episode: 1488, Reward: [2.012564], Mean Reward: -2.53\n",
      "-----------------------------------------------\n",
      "Episode: 1492, Reward: [-0.8894885], Mean Reward: -2.51\n",
      "-----------------------------------------------\n",
      "Episode: 1496, Reward: [-6.621462], Mean Reward: -2.55\n",
      "-----------------------------------------------\n",
      "Episode: 1500, Reward: [-1.2920542], Mean Reward: -2.6\n",
      "-----------------------------------------------\n",
      "Episode: 1504, Reward: [-3.7639835], Mean Reward: -2.6\n",
      "-----------------------------------------------\n",
      "Episode: 1508, Reward: [-3.6821349], Mean Reward: -2.62\n",
      "-----------------------------------------------\n",
      "Episode: 1512, Reward: [-1.2065043], Mean Reward: -2.61\n",
      "-----------------------------------------------\n",
      "Episode: 1516, Reward: [-1.3191923], Mean Reward: -2.61\n",
      "-----------------------------------------------\n",
      "Episode: 1520, Reward: [-2.2939367], Mean Reward: -2.63\n",
      "-----------------------------------------------\n",
      "Episode: 1524, Reward: [0.3100685], Mean Reward: -2.58\n",
      "-----------------------------------------------\n",
      "Episode: 1528, Reward: [1.3036137], Mean Reward: -2.52\n",
      "-----------------------------------------------\n",
      "Episode: 1532, Reward: [-1.0814071], Mean Reward: -2.51\n",
      "-----------------------------------------------\n",
      "Episode: 1536, Reward: [1.2173469], Mean Reward: -2.48\n",
      "-----------------------------------------------\n",
      "Episode: 1540, Reward: [-3.6858532], Mean Reward: -2.41\n",
      "-----------------------------------------------\n",
      "Episode: 1544, Reward: [-2.0361872], Mean Reward: -2.42\n",
      "-----------------------------------------------\n",
      "Episode: 1548, Reward: [-4.7120104], Mean Reward: -2.4\n",
      "-----------------------------------------------\n",
      "Episode: 1552, Reward: [-4.589845], Mean Reward: -2.42\n",
      "-----------------------------------------------\n",
      "Episode: 1556, Reward: [-5.346465], Mean Reward: -2.42\n",
      "-----------------------------------------------\n",
      "Episode: 1560, Reward: [-0.59869325], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1564, Reward: [-3.3653393], Mean Reward: -2.4\n",
      "-----------------------------------------------\n",
      "Episode: 1568, Reward: [-3.8463392], Mean Reward: -2.41\n",
      "-----------------------------------------------\n",
      "Episode: 1572, Reward: [-3.1273255], Mean Reward: -2.43\n",
      "-----------------------------------------------\n",
      "Episode: 1576, Reward: [-5.2276306], Mean Reward: -2.43\n",
      "-----------------------------------------------\n",
      "Episode: 1580, Reward: [-4.865828], Mean Reward: -2.5\n",
      "-----------------------------------------------\n",
      "Episode: 1584, Reward: [-7.5262856], Mean Reward: -2.54\n",
      "-----------------------------------------------\n",
      "Episode: 1588, Reward: [-5.0902343], Mean Reward: -2.59\n",
      "-----------------------------------------------\n",
      "Episode: 1592, Reward: [-6.029065], Mean Reward: -2.63\n",
      "-----------------------------------------------\n",
      "Episode: 1596, Reward: [-0.5767182], Mean Reward: -2.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Episode: 1600, Reward: [-1.5237343], Mean Reward: -2.64\n",
      "-----------------------------------------------\n",
      "Episode: 1604, Reward: [-1.8695205], Mean Reward: -2.61\n",
      "-----------------------------------------------\n",
      "Episode: 1608, Reward: [-2.106791], Mean Reward: -2.59\n",
      "-----------------------------------------------\n",
      "Episode: 1612, Reward: [-0.39751536], Mean Reward: -2.62\n",
      "-----------------------------------------------\n",
      "Episode: 1616, Reward: [-2.933504], Mean Reward: -2.61\n",
      "-----------------------------------------------\n",
      "Episode: 1620, Reward: [-0.31391096], Mean Reward: -2.6\n",
      "-----------------------------------------------\n",
      "Episode: 1624, Reward: [1.0730436], Mean Reward: -2.55\n",
      "-----------------------------------------------\n",
      "Episode: 1628, Reward: [-1.1249435], Mean Reward: -2.51\n",
      "-----------------------------------------------\n",
      "Episode: 1632, Reward: [-2.434177], Mean Reward: -2.49\n",
      "-----------------------------------------------\n",
      "Episode: 1636, Reward: [-3.4540944], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1640, Reward: [-2.062564], Mean Reward: -2.49\n",
      "-----------------------------------------------\n",
      "Episode: 1644, Reward: [-3.9346812], Mean Reward: -2.46\n",
      "-----------------------------------------------\n",
      "Episode: 1648, Reward: [1.1393862], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1652, Reward: [1.6956489], Mean Reward: -2.4\n",
      "-----------------------------------------------\n",
      "Episode: 1656, Reward: [-2.5963557], Mean Reward: -2.4\n",
      "-----------------------------------------------\n",
      "Episode: 1660, Reward: [-0.30497342], Mean Reward: -2.39\n",
      "-----------------------------------------------\n",
      "Episode: 1664, Reward: [-3.001446], Mean Reward: -2.4\n",
      "-----------------------------------------------\n",
      "Episode: 1668, Reward: [-5.5836043], Mean Reward: -2.41\n",
      "-----------------------------------------------\n",
      "Episode: 1672, Reward: [1.9153092], Mean Reward: -2.42\n",
      "-----------------------------------------------\n",
      "Episode: 1676, Reward: [-2.0688481], Mean Reward: -2.36\n",
      "-----------------------------------------------\n",
      "Episode: 1680, Reward: [-4.3315296], Mean Reward: -2.34\n",
      "-----------------------------------------------\n",
      "Episode: 1684, Reward: [0.3506083], Mean Reward: -2.36\n",
      "-----------------------------------------------\n",
      "Episode: 1688, Reward: [-5.304953], Mean Reward: -2.32\n",
      "-----------------------------------------------\n",
      "Episode: 1692, Reward: [-2.7559123], Mean Reward: -2.32\n",
      "-----------------------------------------------\n",
      "Episode: 1696, Reward: [-5.351907], Mean Reward: -2.3\n",
      "-----------------------------------------------\n",
      "Episode: 1700, Reward: [-3.7574425], Mean Reward: -2.31\n",
      "-----------------------------------------------\n",
      "Episode: 1704, Reward: [-3.7236047], Mean Reward: -2.35\n",
      "-----------------------------------------------\n",
      "Episode: 1708, Reward: [-0.24933946], Mean Reward: -2.33\n",
      "-----------------------------------------------\n",
      "Episode: 1712, Reward: [-0.08065766], Mean Reward: -2.31\n",
      "-----------------------------------------------\n",
      "Episode: 1716, Reward: [-4.5823555], Mean Reward: -2.32\n",
      "-----------------------------------------------\n",
      "Episode: 1720, Reward: [0.92287904], Mean Reward: -2.34\n",
      "-----------------------------------------------\n",
      "Episode: 1724, Reward: [-1.9189178], Mean Reward: -2.29\n",
      "-----------------------------------------------\n",
      "Episode: 1728, Reward: [-5.1444073], Mean Reward: -2.31\n",
      "-----------------------------------------------\n",
      "Episode: 1732, Reward: [3.1041195], Mean Reward: -2.35\n",
      "-----------------------------------------------\n",
      "Episode: 1736, Reward: [-10.482364], Mean Reward: -2.29\n",
      "-----------------------------------------------\n",
      "Episode: 1740, Reward: [-2.9073393], Mean Reward: -2.39\n",
      "-----------------------------------------------\n",
      "Episode: 1744, Reward: [-0.73460096], Mean Reward: -2.44\n",
      "-----------------------------------------------\n",
      "Episode: 1748, Reward: [-7.688688], Mean Reward: -2.4\n",
      "-----------------------------------------------\n",
      "Episode: 1752, Reward: [-4.5685415], Mean Reward: -2.43\n",
      "-----------------------------------------------\n",
      "Episode: 1756, Reward: [-2.1578207], Mean Reward: -2.41\n",
      "-----------------------------------------------\n",
      "Episode: 1760, Reward: [-1.5664234], Mean Reward: -2.41\n",
      "-----------------------------------------------\n",
      "Episode: 1764, Reward: [-3.5347009], Mean Reward: -2.39\n",
      "-----------------------------------------------\n",
      "Episode: 1768, Reward: [-2.9559655], Mean Reward: -2.45\n",
      "-----------------------------------------------\n",
      "Episode: 1772, Reward: [-4.4678698], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1776, Reward: [-3.6902835], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1780, Reward: [2.1179354], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1784, Reward: [-2.2098258], Mean Reward: -2.42\n",
      "-----------------------------------------------\n",
      "Episode: 1788, Reward: [-7.3377767], Mean Reward: -2.43\n",
      "-----------------------------------------------\n",
      "Episode: 1792, Reward: [-1.5681999], Mean Reward: -2.5\n",
      "-----------------------------------------------\n",
      "Episode: 1796, Reward: [-3.5749018], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1800, Reward: [-3.5365145], Mean Reward: -2.51\n",
      "-----------------------------------------------\n",
      "Episode: 1804, Reward: [-4.582967], Mean Reward: -2.48\n",
      "-----------------------------------------------\n",
      "Episode: 1808, Reward: [-0.18177909], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1812, Reward: [-6.207319], Mean Reward: -2.42\n",
      "-----------------------------------------------\n",
      "Episode: 1816, Reward: [-4.7024846], Mean Reward: -2.47\n",
      "-----------------------------------------------\n",
      "Episode: 1820, Reward: [-5.896969], Mean Reward: -2.49\n",
      "-----------------------------------------------\n",
      "Episode: 1824, Reward: [-6.740694], Mean Reward: -2.52\n",
      "-----------------------------------------------\n",
      "Episode: 1828, Reward: [-10.441339], Mean Reward: -2.61\n",
      "-----------------------------------------------\n",
      "Episode: 1832, Reward: [-3.2596025], Mean Reward: -2.67\n",
      "-----------------------------------------------\n",
      "Episode: 1836, Reward: [-0.9892428], Mean Reward: -2.73\n",
      "-----------------------------------------------\n",
      "Episode: 1840, Reward: [2.7186635], Mean Reward: -2.73\n",
      "-----------------------------------------------\n",
      "Episode: 1844, Reward: [2.0754824], Mean Reward: -2.64\n",
      "-----------------------------------------------\n",
      "Episode: 1848, Reward: [-3.1936753], Mean Reward: -2.59\n",
      "-----------------------------------------------\n",
      "Episode: 1852, Reward: [-4.56029], Mean Reward: -2.64\n",
      "-----------------------------------------------\n",
      "Episode: 1856, Reward: [-4.6118374], Mean Reward: -2.65\n",
      "-----------------------------------------------\n",
      "Episode: 1860, Reward: [-3.9384751], Mean Reward: -2.66\n",
      "-----------------------------------------------\n",
      "Episode: 1864, Reward: [-1.9049999], Mean Reward: -2.69\n",
      "-----------------------------------------------\n",
      "Episode: 1868, Reward: [-4.873019], Mean Reward: -2.71\n",
      "-----------------------------------------------\n",
      "Episode: 1872, Reward: [-3.998835], Mean Reward: -2.73\n",
      "-----------------------------------------------\n",
      "Episode: 1876, Reward: [-4.0949626], Mean Reward: -2.73\n",
      "-----------------------------------------------\n",
      "Episode: 1880, Reward: [-5.0610104], Mean Reward: -2.76\n",
      "-----------------------------------------------\n",
      "Episode: 1884, Reward: [-4.9599714], Mean Reward: -2.79\n",
      "-----------------------------------------------\n",
      "Episode: 1888, Reward: [-4.2661], Mean Reward: -2.81\n",
      "-----------------------------------------------\n",
      "Episode: 1892, Reward: [-1.5175734], Mean Reward: -2.87\n",
      "-----------------------------------------------\n",
      "Episode: 1896, Reward: [-3.8749726], Mean Reward: -2.88\n",
      "-----------------------------------------------\n",
      "Episode: 1900, Reward: [-2.9647083], Mean Reward: -2.85\n",
      "-----------------------------------------------\n",
      "Episode: 1904, Reward: [-1.262327], Mean Reward: -2.87\n",
      "-----------------------------------------------\n",
      "Episode: 1908, Reward: [0.27057475], Mean Reward: -2.84\n",
      "-----------------------------------------------\n",
      "Episode: 1912, Reward: [-1.1776466], Mean Reward: -2.8\n",
      "-----------------------------------------------\n",
      "Episode: 1916, Reward: [-2.2304652], Mean Reward: -2.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "Episode: 1920, Reward: [-4.5850415], Mean Reward: -2.81\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.functional import mse_loss\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters for training\n",
    "initial_action_noise_stddev = 0.2 # Standard deviation of action noise\n",
    "final_action_noise_stddev = 0.2\n",
    "noise_decay_rate = 1\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "buffer_size = 80000#100000#50000\n",
    "min_samples_before_train = 500\n",
    "update_every = 5\n",
    "updates_per_step = 1\n",
    "target_update_freq = 50  # Update target networks every 10 updates\n",
    "# Initialize target entropy\n",
    "target_entropy = -action_dim\n",
    "# Initialize entropy coefficient alpha\n",
    "log_alpha = torch.tensor(np.log(0.2), requires_grad=True, device=device)\n",
    "alpha_optimizer = optim.Adam([log_alpha], lr=3e-4)\n",
    "\n",
    "# Initialize global variables\n",
    "update_counter = 0\n",
    "# Lists to store data for plotting\n",
    "episode_rewards = []\n",
    "mean_reward = []\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "actions = []\n",
    "\n",
    "\n",
    "# Function to update action noise\n",
    "def get_action_noise_stddev(episode):\n",
    "    return max(final_action_noise_stddev, initial_action_noise_stddev * (noise_decay_rate ** episode))\n",
    "\n",
    "# Plotting Moving Averages\n",
    "def plot_moving_average(data, window_size=100):\n",
    "    moving_avg = np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "    return moving_avg\n",
    "\n",
    "def flatten_state(state_dict):\n",
    "    state_array = []\n",
    "    for key, value in state_dict.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            value[np.isnan(value)] = 0.0  # Replace NaNs with 0\n",
    "            state_array.extend(value.flatten())\n",
    "        elif isinstance(value, int) or isinstance(value, float):\n",
    "            if np.isnan(value):\n",
    "                value = 0.0  # Replace NaNs with 0\n",
    "            state_array.append(value)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported state type for key {key}: {type(value)}\")\n",
    "    return np.array(state_array)\n",
    "\n",
    "\n",
    "def normalize_state(state):\n",
    "    for key, value in state.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            state[key] = (value - value.mean()) / (value.std() + 1e-8)\n",
    "        elif isinstance(value, (int, float)):\n",
    "            if key == 'current_day':\n",
    "                state[key] = (value - 182.5) / 105.0\n",
    "            elif key == 'current_month':\n",
    "                state[key] = (value - 6.5) / 3.5\n",
    "            elif key == 'current_month_day':\n",
    "                state[key] = (value - 15.5) / 9.0\n",
    "            elif key == 'current_year':\n",
    "                state[key] = (value - 1500) / 750\n",
    "            else:\n",
    "                state[key] = (value - 0.5)  # Example normalization for binary states\n",
    "    return state\n",
    "\n",
    "def clip_reward(reward, min_reward, max_reward):\n",
    "    return max(min(reward, max_reward), min_reward)\n",
    "\n",
    "\n",
    "# Experience replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Instantiate the replay buffer\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "\n",
    "# Function to update the model\n",
    "def update_model():\n",
    "    global update_counter  # Declare update_counter as global to modify it within the function\n",
    "    if len(replay_buffer) < min_samples_before_train:\n",
    "        return\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "    states = torch.FloatTensor(states).to(device)\n",
    "    actions = torch.FloatTensor(actions).to(device)\n",
    "    rewards = torch.FloatTensor(rewards).to(device)\n",
    "    next_states = torch.FloatTensor(next_states).to(device)\n",
    "    dones = torch.FloatTensor(dones).to(device)\n",
    "\n",
    "    # Predict actions and log probabilities with the current policy\n",
    "    new_actions, log_probs = actor(states)\n",
    "    next_actions, next_log_probs = actor(next_states)\n",
    "    \n",
    "    for name, param in actor.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(f\"NaN gradient in {name}\")\n",
    "    \n",
    "    # Calculate the target Q values\n",
    "    next_q1 = critic1_target(next_states, next_actions.detach())\n",
    "    next_q2 = critic2_target(next_states, next_actions.detach())\n",
    "    next_v = torch.min(next_q1, next_q2) - alpha * next_log_probs.detach()\n",
    "    expected_q = rewards + (1 - dones) * gamma * next_v\n",
    "\n",
    "    # Calculate the losses for both critics\n",
    "    current_q1 = critic1(states, actions)\n",
    "    current_q2 = critic2(states, actions)\n",
    "    critic1_loss = F.mse_loss(current_q1, expected_q)\n",
    "    critic2_loss = F.mse_loss(current_q2, expected_q)\n",
    "    combined_loss = critic1_loss + critic2_loss\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    critic1_optimizer.zero_grad()\n",
    "    critic2_optimizer.zero_grad()\n",
    "    combined_loss.backward()\n",
    "    # Clip gradients to prevent explosion\n",
    "    nn.utils.clip_grad_norm_(critic1.parameters(), max_norm=max_grad_norm)\n",
    "    nn.utils.clip_grad_norm_(critic2.parameters(), max_norm=max_grad_norm)\n",
    "    \n",
    "    critic1_optimizer.step()\n",
    "    critic2_optimizer.step()\n",
    "\n",
    "     # Delayed policy updates\n",
    "    actor_loss = -(torch.min(critic1(states, new_actions), critic2(states, new_actions)) - alpha * log_probs).mean()\n",
    "    \n",
    "    actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    nn.utils.clip_grad_norm_(actor.parameters(), max_norm=max_grad_norm)\n",
    "\n",
    "    actor_optimizer.step()\n",
    "    \n",
    "    # Update alpha\n",
    "    alpha_loss = -(log_alpha * (log_probs + target_entropy).detach()).mean()\n",
    "    alpha_optimizer.zero_grad()\n",
    "    alpha_loss.backward()\n",
    "    alpha_optimizer.step()\n",
    "\n",
    "    # Less frequent target network updates\n",
    "    if update_counter % target_update_freq == 0:\n",
    "         # Soft update target networks\n",
    "        for param, target_param in zip(critic1.parameters(), critic1_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        for param, target_param in zip(critic2.parameters(), critic2_target.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    update_counter += 1\n",
    "    \n",
    "    return combined_loss, actor_loss\n",
    "\n",
    "\n",
    "def normalize_reward(reward, reward_mean, reward_std):\n",
    "    #print(\"\\nREWARD --- :\", reward)\n",
    "    #print(\"\\nMEAN REWARD --- :\", reward_mean)\n",
    "    #print(\"\\nREWARD STD --- :\", reward_std, \"\\n\")\n",
    "    return (reward - reward_mean) / (reward_std + 1e-8)\n",
    "\n",
    "reward_buffer = []\n",
    "reward_mean = 0\n",
    "reward_std = 1\n",
    "\n",
    "# Update the reward normalization statistics\n",
    "def update_reward_stats(reward):\n",
    "    global reward_mean, reward_std\n",
    "    reward_buffer.append(reward)\n",
    "    if len(reward_buffer) > 1000:\n",
    "        reward_buffer.pop(0)\n",
    "    reward_mean = np.mean(reward_buffer)\n",
    "    reward_std = np.std(reward_buffer)\n",
    "\n",
    "\n",
    "def check_nan_in_state(state):\n",
    "    \"\"\"Recursively check for NaN values in the state dictionary.\"\"\"\n",
    "    if isinstance(state, dict):\n",
    "        for key, value in state.items():\n",
    "            if check_nan_in_state(value):\n",
    "                return True\n",
    "    elif isinstance(state, np.ndarray):\n",
    "        if np.isnan(state).any():\n",
    "            return True\n",
    "    elif isinstance(state, (float, int)):  # Directly check if it's a scalar\n",
    "        if np.isnan(state):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_episodes = 2000\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    if check_nan_in_state(state):\n",
    "        print(\"Skipping episode due to NaN in initial state\")\n",
    "        continue\n",
    "        \n",
    "    state = normalize_state(state)  # Normalize the state\n",
    "    state = flatten_state(state)  # Ensure the state is correctly flattened\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).to(device)\n",
    "        # Before passing data to the network\n",
    "        if torch.isnan(state_tensor).any() or torch.isinf(state_tensor).any():\n",
    "            raise ValueError(\"State tensor contains NaN or Inf.\")\n",
    "        \n",
    "        action, log_prob = actor(state_tensor.unsqueeze(0))  # Ensure it's treated as a batch of one\n",
    "        action = action.squeeze(0).detach().cpu().numpy()  # Remove batch dimension before stepping the environment\n",
    "        action_noise_stddev = get_action_noise_stddev(episode)\n",
    "        # Fixed action noise\n",
    "        action_noise_stddev = initial_action_noise_stddev\n",
    "        \n",
    "        # Add fixed noise to action for exploration\n",
    "        action = np.clip(action + np.random.normal(0, action_noise_stddev, size=action.shape), 0, action_bound)\n",
    "        actions.append(action)\n",
    "        \n",
    "        #print(f'-------------In episode {episode}\\n')\n",
    "        #print(f'Selected Action: {action}\\n ')\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        reward = normalize_reward(reward, reward_mean, reward_std)  # Normalize reward\n",
    "        update_reward_stats(reward)  # Update reward statistics\n",
    "        \n",
    "#         print(f'Reward: {reward} - Growth Stage: {next_state[\"growth_stage\"]} - Harvest: {next_state[\"harvest\"]} - Water Needs {next_state[\"water_needs\"]} -Excess: {next_state[\"accumulated_excess\"]} -  Scarcity: {next_state[\"accumulated_scarcity\"]} - ')\n",
    "#         print(f'\\nIs Done: {done}')\n",
    "        \n",
    "        next_state = normalize_state(next_state)  # Normalize the next state\n",
    "        next_state = flatten_state(next_state)\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        \n",
    "        if len(replay_buffer) > min_samples_before_train:\n",
    "            critic_loss, actor_loss = update_model()\n",
    "#             if episode % 80 == 1:\n",
    "#                 critic_losses.append(critic_loss)\n",
    "#                 actor_losses.append(actor_loss)    \n",
    "\n",
    "    if episode % 4 == 0:\n",
    "        episode_rewards.append(episode_reward)\n",
    "        if len(episode_rewards[-101:-1]) == 0:\n",
    "            ep_mean_rew = -np.inf\n",
    "        else: \n",
    "            ep_mean_rew = round(float(np.mean(episode_rewards[-101:-1])), 2)\n",
    "        mean_reward.append(ep_mean_rew)\n",
    "        if len(replay_buffer) > min_samples_before_train:\n",
    "            critic_losses.append(critic_loss)\n",
    "            actor_losses.append(actor_loss)\n",
    "        print(\"-----------------------------------------------\")\n",
    "        print(f\"Episode: {episode}, Reward: {episode_reward}, Mean Reward: {ep_mean_rew}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b5e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards = np.mean(episode_rewards)\n",
    "print(mean_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a99874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, plot the collected data\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot episode rewards\n",
    "plt.plot(episode_rewards)\n",
    "plt.title('Episode Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, plot the collected data\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot episode rewards\n",
    "plt.plot(mean_reward)\n",
    "plt.title('Episode Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffa9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, plot the collected data\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot episode rewards\n",
    "#plt.subplot(1, 2, 1)\n",
    "plt.plot(actions)\n",
    "plt.title('Episode Actions')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Actions')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9974484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting values for plotting\n",
    "actor_losses_values = [tensor.item() for tensor in actor_losses]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actor_losses_values, label='Actor Losses')\n",
    "plt.plot(plot_moving_average(actor_losses_values), label='Moving Average Actor Losses', linewidth=2)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Actor Losses Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6a155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(actor_losses_values, marker='o', linestyle='-', color='b')\n",
    "plt.title('Actor Losses Over Time')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa88dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "critic_losses_values = [tensor.item() for tensor in critic_losses]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(critic_losses_values, marker='o', linestyle='-', color='b')\n",
    "plt.title('Critic Losses Over Time')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(critic_losses_values, label='Critic Losses')\n",
    "plt.plot(plot_moving_average(critic_losses_values), label='Moving Average Critic Losses', linewidth=2)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.title('Critic Losses Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "# Test the trained model and calculate mean reward\n",
    "def test_model(env, actor, num_episodes=100):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = normalize_state(state)  # Normalize the state\n",
    "        state = flatten_state(state)  # Ensure the state is correctly flattened\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).to(device)\n",
    "            action, _ = actor(state_tensor.unsqueeze(0))  # Ensure it's treated as a batch of one\n",
    "            action = action.squeeze(0).detach().cpu().numpy()  # Remove batch dimension before stepping the environment\n",
    "            action = np.clip(action, 0, action_space.high)  # Clip action to the valid range\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = normalize_state(next_state)  # Normalize the next state\n",
    "            next_state = flatten_state(next_state)\n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"Episode: {episode}, Reward: {episode_reward}\")\n",
    "    \n",
    "    test_mean_reward = np.mean(total_rewards)\n",
    "    print(f\"Mean Reward over {num_episodes} episodes: {test_mean_reward}\")\n",
    "    return test_mean_reward\n",
    "\n",
    "# Assuming the actor is already loaded and env is defined\n",
    "test_mean_reward = test_model(env, actor, num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, plot the collected data\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot episode rewards\n",
    "plt.plot(total_rewards)\n",
    "plt.title('Episode Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to flatten a list of arrays\n",
    "def flatten_list(data):\n",
    "    return [item.flatten()[0] for item in data]\n",
    "\n",
    "# Helper function to convert tensors to floats\n",
    "def tensor_to_float(data):\n",
    "    return [item.item() if isinstance(item, torch.Tensor) else item for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebe22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each list as a separate CSV file\n",
    "def save_list_to_csv(data, filename):\n",
    "    # Convert tensors to floats if necessary\n",
    "    if isinstance(data, (list, np.ndarray)) and isinstance(data[0], (torch.Tensor, np.ndarray, list)):\n",
    "        data = tensor_to_float(data)\n",
    "    if isinstance(data, (list, np.ndarray)) and isinstance(data[0], (np.ndarray, list)):\n",
    "        data = flatten_list(data)\n",
    "    df = pd.DataFrame({filename: data})\n",
    "    df.to_csv(f'{filename}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_list_to_csv(episode_rewards, 'episode_rewards')\n",
    "save_list_to_csv(mean_reward, 'mean_reward_fixed_noise')\n",
    "save_list_to_csv(critic_losses, 'critic_losses_fixed_noise')\n",
    "save_list_to_csv(actor_losses, 'actor_losses_fixed_noise')\n",
    "#save_list_to_csv(actions, 'actions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5369f2",
   "metadata": {},
   "source": [
    "# Example Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.0, 2.0, 3.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79afcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_v1 = Attention(input_dim=3, attention_dim=2)\n",
    "\n",
    "# Forward pass\n",
    "output_v1, weights_v1 = attention_v1(x)\n",
    "print(\"AttentionV1 Output:\", output_v1)\n",
    "print(\"AttentionV1 Weights:\", weights_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca45e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_v1.fc1.weight.data = torch.tensor([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
    "attention_v1.fc1.bias.data = torch.tensor([0.1, 0.2])\n",
    "attention_v1.fc2.weight.data = torch.tensor([[0.1], [0.2]])\n",
    "attention_v1.fc2.bias.data = torch.tensor([0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803c3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1 = F.relu(x @ attention_v1.fc1.weight.T + attention_v1.fc1.bias)\n",
    "print(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c637cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = scores1 @ attention_v1.fc2.weight + attention_v1.fc2.bias\n",
    "print(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = F.softmax(scores2, dim=1)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = (x * weights).sum(dim=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa8c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
